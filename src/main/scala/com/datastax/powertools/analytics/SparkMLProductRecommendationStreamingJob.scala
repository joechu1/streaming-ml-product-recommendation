package com.datastax.powertools.analytics

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.{Seconds, StreamingContext}
import com.datastax.powertools.analytics.ddl.DSECapable
import org.apache.spark.ml.recommendation.ALS
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{SQLContext, SaveMode, SparkSession}
import org.apache.spark.sql.cassandra._

import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe

import org.apache.log4j.{Level, Logger}

// For DSE it is not necessary to set connection parameters for spark.master (since it will be done
// automatically)

/**
 * https://github.com/brkyvz/streaming-matrix-factorization
 * https://issues.apache.org/jira/browse/SPARK-6407
 */
object SparkMLProductRecommendationStreamingJob extends DSECapable{

  def main(args: Array[String]) {
    if (args.length < 2) {
      System.err.println("Usage: SparkMLProductRecommendation <hostname> <port>")
      System.exit(1)
    }

    object Holder extends Serializable {      
      @transient lazy val log = Logger.getLogger(getClass.getName)    
    }
    Holder.log.info("Connecting to DSE")

    // Create the context with a 1 second batch size
    val sc = connectToDSE("SparkMLProductRecommendation")

    Holder.log.info("Setting up recommendations schema")

    println("Before setting up schema")
    // Set up schema
    setupSchema("recommendations", "predictions", "(user int, item int, preference float, prediction float, PRIMARY KEY((user), item))")

    //Start sql context to read flat file
    val sqlContext = new SQLContext(sc)

    /*
    //train with batch file:
    Holder.log.info("Training ML with batch file")
    //get the raw data
    val trainingData = sqlContext.read.format("com.databricks.spark.csv")
      .option("header", "true")
      .option("inferSchema", "true")
      .option("delimiter", ":")
      .load("dsefs:///sales_observations")
      .cache()

    //Instantiate our estimator
    val algorithm = new ALS()
      .setMaxIter(5)
      .setRegParam(0.01)
      .setImplicitPrefs(true)
      .setUserCol("user")
      .setItemCol("item")
      .setRatingCol("preference")

    //val algorithm = new LatentMatrixFactorization()
    //train the Estimator against the training data from the file to produce a trained Transformer.
    val model = algorithm.fit(trainingData)
   */
    val ssc = new StreamingContext(sc, Seconds(5))

    // Create a socket stream on target ip:port and count the
    // words in input stream of \n delimited text (eg. generated by 'nc')
    // Note that no duplication in storage level only for running locally.
    // Replication necessary in distributed scenario for fault tolerance.
    // ==================================================================
    // Change to use Kafka
    println("Before setting up Kafka")
    Holder.log.info("Setting up Kafka connection")

    val kafkaParams = Map[String, Object](
      "bootstrap.servers" -> (args(0) + ":" + args(1)),
      "key.deserializer" -> classOf[StringDeserializer],
      "value.deserializer" -> classOf[StringDeserializer],
      "group.id" -> "1",
      "auto.offset.reset" -> "latest",
      "enable.auto.commit" -> (false: java.lang.Boolean)
    ) 
    val topics = Array("product-ratings")
    val stream = KafkaUtils.createDirectStream[String, String](
      ssc,
      PreferConsistent,
      Subscribe[String, String](topics, kafkaParams)
    )

    println("right before observation")
    val observation = stream.map(record => record.value)

    val testStream = observation.map(_.split(":")).filter(x => {
      (x.size >2 && x(0).forall(_.isDigit))
    }).map(x =>
      (x(0).toLong, x(1).toLong, x(2).toFloat)
    )

    //now predict against the live stream
    //this gives us predicted ratings for the item user combination fed from the stream
    //words.foreachRDD { (rdd: RDD[String], time: Time) =>
    testStream.foreachRDD { (rdd:RDD[(Long, Long, Float)], time: org.apache.spark.streaming.Time) =>
      // Get the singleton instance of SparkSession
      val spark = SparkSessionSingleton.getInstance(rdd.sparkContext.getConf)
      import spark.implicits._

      // Convert RDD[String] to RDD[case class] to DataFrame

      //val testStreamDF = rdd.map((x, y, z) => Rating((Long)x, (Long)y, (Float)z)).toDF()
      val testStreamDS = rdd.map((r:(Long,Long,Float)) => Rating(r._1, r._2, r._3)).toDS()

      //val predictions = model.transform(testStreamDS).cache();

      //predictions.write.cassandraFormat("predictions", "recommendations").mode(SaveMode.Append).save

      //predictions.map(x => "item: " + x.getLong(0)+ " user: " +  x.getLong(1) + " rating: " + x.getFloat(2)).show()
    }


    observation.foreachRDD(row => {
      print(s"Cached the predictions table")
    })

    ssc.start()
    ssc.awaitTermination()
  }

  var conf: SparkConf = _
  var sc: SparkContext = _
}

// scalastyle:on println
